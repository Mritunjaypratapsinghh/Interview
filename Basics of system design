System Design – Detailed Notes
1. Introduction to System Design

System Design is the art of building scalable, highly available, and fault-tolerant systems.
These notes explain how different components work together in a production environment.

Topics Covered

Load Balancers

API Gateways

Caching Layers

Queue Systems & Asynchronous Processing

Microservice Architecture

Cloud Concepts

Horizontal & Vertical Scaling

Rate Limiting

CDN (Content Delivery Networks)

2. Basic Architecture — Client and Server
1) Client

Mobile phones, laptops, tablets, IoT devices

Sends HTTP requests to the server

2) Server

Machine with a public IP, runs 24/7

Cloud servers (AWS EC2, DigitalOcean, etc.)

3. DNS (Domain Name System)

DNS maps domain names → IP addresses.
Example: amazon.com → 13.32.64.0

DNS Resolution Flow

User types domain

Browser contacts DNS

DNS returns IP

Browser sends request to server

4. Problem: Server Overload

Servers have physical limits (CPU, RAM).
High traffic causes:

Out-of-memory errors

Crashes

High latency

5. Vertical Scaling (Scale Up)

Increase power of a single server:

Add more CPU

Add more RAM

Add more Disk

Pros

Simple

No code change required

Cons

Expensive

Hardware limits

Requires downtime

Single point of failure

6. Horizontal Scaling (Scale Out)

Instead of upgrading one machine, add multiple machines.

Advantages

Zero downtime

Nearly infinite scalability

Fault tolerance

Cost effective

Problem

Multiple servers = multiple IPs → need a Load Balancer

7. Load Balancer
Responsibilities

Distribute traffic

Monitor health

Route only to healthy servers

Expose single public IP

Load Balancing Algorithms

Round Robin

Least Connections

Weighted Routing

8. Microservices Architecture

Break system into independent services:

Auth Service

Orders Service

Payments Service

Each service:

Has its own load balancer

Has multiple replicas

Scales independently

9. API Gateway

A single unified entry point.

Responsibilities

Path-based routing

Authentication

Rate limiting

Logging & monitoring

10. Synchronous vs Asynchronous Processing
Synchronous

Caller waits → slow for tasks like:

Sending emails

Bulk CSV uploads

Asynchronous

Use Queues + Workers → non-blocking processing.

11. Queues (SQS / RabbitMQ / Kafka)

Queues:

Store messages FIFO

Decouple producers/consumers

Queue Polling

Short Polling → frequent, expensive

Long Polling → efficient, fewer API calls

Workers can be rate-limited to avoid external API limits.

12. Pub/Sub Model (SNS)

One-to-many messaging.

Example:
A payment event triggers:

Email service

SMS service

WhatsApp notifications

Analytics

13. Fan-Out Architecture

SNS → multiple SQS queues → multiple workers.

Best for:

Notifications

Multi-service triggers

Video processing pipelines (like YouTube)

14. Dead Letter Queue (DLQ)

When a message repeatedly fails:

It is moved to DLQ

Can be retried later

Prevents message loss

15. Rate Limiting

Prevents abuse & DDOS.

Common Algorithms

Token Bucket

Leaky Bucket

Sliding Window

Applied at:

API Gateway

Load Balancer

Individual services

16. Caching Layer (Redis)
Flow

Check Redis

If cached → return

Else → query DB, store result in Redis

Reduces DB load drastically.

17. Database Scaling
1) Read Replicas

Offload read-heavy workloads

2) Primary DB

Handles all writes

Strong consistency

3) Sharding

Split DB by:

User ID

Tenant ID

Region

18. CDN (e.g., CloudFront)

Caches static content at global edge locations.

Flow

User requests asset

CDN checks local cache

If not found → fetch from origin → cache → return

Reduces:

Latency

Bandwidth cost

Server load

19. Complete System Summary

A scalable, production-grade system includes:

DNS

CDN

API Gateway

Load Balancer

Microservices

Autoscaling

Queues

Pub/Sub

Database Scaling (Replicas + Sharding)

Cache

Monitoring

Rate Limiting

20. Final Understanding

By mastering:

LB

API Gateway

Cache

Queue

Pub/Sub

CDN

Scaling

Microservices

You can design systems like:
Amazon, Netflix, Uber, Flipkart.
